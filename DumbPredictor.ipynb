{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dumb Predictor\n",
    "#### Sam Berkson\n",
    "#### CPSC 323"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this assignment was to create a dumb predictor, a classifier that predicts the most common class label, and run it over a real life skewed dataset.  I chose the Titanic dataset, a list of attributes for every passenger on the Titanic during its last voyage.  I ran my model over the 'survived' attribute, predicting whether someone survived the journey."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I need to read in the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I created a class containing my dumb predictor.  It has a constructor and a predict method.  The predict method takes in the data as a list, and finds the most commonly occuring value.  It then appends this value into the predictions list X number of times, where X is the size of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DumbPredictor:\n",
    "    \"Object used to fit data to a dumb prediction model, where the predicted value on unseen instances is the most commonly occuring value in the training set.\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"Initializes the DumbPredictor object.\"\n",
    "        self.most_common = None\n",
    "        self.data = None\n",
    "    \n",
    "    # Find most common class label to use as prediction for all instances\n",
    "    def predict(self, data):\n",
    "        start = time.time()\n",
    "        self.data = data\n",
    "        self.most_common = max(set(self.data), key=self.data.count)\n",
    "\n",
    "        predictions = []\n",
    "        for i in range(len(self.data)):\n",
    "            predictions.append(self.most_common)\n",
    "        end = time.time()\n",
    "        return predictions, (end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I read in my dataset and separated the 'survived' column into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dtype=None)\n",
    "df = pd.read_csv('titanic.csv')\n",
    "\n",
    "# Create a list of the target values\n",
    "input = df['survived'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I instantiated my model and passed in my input list.  I then ran a parallel analysis of both the input and the predictions, finding which were correctly and incorrectly identified and incrementing counter variables for each metric (FP, TP, FN, TN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DumbPredictor()\n",
    "results, time = model.predict(input)\n",
    "\n",
    "Positive, Negative, TP, FP, TN, FN = 0, 0, 0, 0, 0, 0\n",
    "\n",
    "for index, val in enumerate(results):\n",
    "    if results[index] == 'yes':\n",
    "        Positive += 1\n",
    "        if results[index] and input[index] == 'yes':\n",
    "            TP += 1\n",
    "        else:\n",
    "            FP += 1\n",
    "    elif results[index] == 'no':\n",
    "        Negative += 1\n",
    "        if results[index] and input[index] == 'no':\n",
    "            TN += 1\n",
    "        else:\n",
    "            FN += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I will analyze my model on 5 metrics:\n",
    "* Accuracy\n",
    "* Precision\n",
    "* Recall\n",
    "* F1\n",
    "* Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6769650159018628\n",
      "Precision:  0.6769650159018628\n",
      "Recall:  1.0\n",
      "F1:  0.8073692766188024\n",
      "Time:  0.00021600723266601562\n",
      "\n",
      "Confusion Matrix:\n",
      "Total: 2201           | Predicted Survived: 2201 | Predicted Died: 0\n",
      "Actual Survived: 1490 | TP:  1490                | FN:  0  \n",
      "Actual Died: 711      | FP:  711                 | TN:  0  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "Precision = TP / (TP + FP)\n",
    "Recall = TP / (TP + FN)\n",
    "F1 = 2 * ((Precision * Recall) / (Precision + Recall))\n",
    "\n",
    "print(\"Accuracy: \", Accuracy)\n",
    "print(\"Precision: \", Precision)\n",
    "print(\"Recall: \", Recall)\n",
    "print(\"F1: \", F1)\n",
    "print(\"Time: \", time)\n",
    "\n",
    "def confusion_matrix(total, TP, FP, TN, FN):\n",
    "    print(\"Total:\", total, \"          | Predicted Survived:\", Positive, \"| Predicted Died:\", Negative)\n",
    "    print(\"Actual Survived:\", TP + FN, \"| TP: \", TP, \"               | FN: \", FN, \" \")\n",
    "    print(\"Actual Died:\", FP + TN, \"     | FP: \", FP, \"                | TN: \", TN, \" \")\n",
    "    \n",
    "print(\"\\nConfusion Matrix:\")\n",
    "confusion_matrix(len(results), TP, FP, TN, FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, my model performed as well as I expected it to.  'yes' was the most commonly occuring class label, and it occured in exactly 67.695% of instances, matching the accuracy of my model.  It follows that my recall would be one, since I predicted 'yes' for every instance.  My F1 score also looks in like to where it should be, along with the time it took to predict (there are only 2201 instances, it shouldnt take very long to predict).  My confusion matrix looks as expected, matching all of my other metrics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7b639dd3956d1ea949d1be02ef43c9b05c64a6b2f005de22d521c23e3c341cfd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
